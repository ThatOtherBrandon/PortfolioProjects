{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwzFeW44Vaf8rLd+v+THsd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "8RCAmgydkm8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project involves analyzing a provided dataset that contains information about the voting behavior of various counties in the United States. The goal is to use classification methods to predict whether a county will vote \"yes\" or \"no\" to legalizing gaming through a ballot.\n",
        "\n",
        "0 = NO\n",
        "1 = Yes"
      ],
      "metadata": {
        "id": "1bcxJuswkwhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "yGlh6nfek1sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this data preprocessing I took time just getting a feel for the data given and seeing what I wanted and didnt want in my dataframe."
      ],
      "metadata": {
        "id": "vXu8LAUOlM6h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD4W5szQt0n_"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive add some .shapes mf for the screenshot\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "U9I1zpHUuBnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IS470_Data/Gaming Ballot Data Set-1.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "U7ZbA7eEuGM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.keys()"
      ],
      "metadata": {
        "id": "lBurHh5puHQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "yj3syXm2u3fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the desired columns only\n",
        "desired_columns = ['State No','DEPENDENT VARIABLE', 'BALLOT TYPE', 'POPULATION', 'PCI',\n",
        "       'MEDIUM FAMILY INCOME', 'POPULATION DENSITY','PERCENT WHITE', 'PERCENT BLACK', 'PERCENT OTHER', 'PERCENT MALE' , 'POVERTY LEVEL'\n",
        "       ,'UNEMPLOYMENT RATE','AGE LESS THAN 18', 'AGE24', 'AGE44', 'AGE64','AGE OLDER THAN 65', 'MSA']\n",
        "\n",
        "gaming_desired = df[desired_columns]"
      ],
      "metadata": {
        "id": "6zvXjZ9rwacc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = gaming_desired.copy()\n",
        "df.loc[:, 'DEPENDENT VARIABLE'] = df['DEPENDENT VARIABLE'].replace({0: 'No', 1: 'Yes'})"
      ],
      "metadata": {
        "id": "T9rZBeNS2OSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "erhxW-I72Q4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this part I removed the $ and the , so I wont run into errors in the future.\n",
        "df['MEDIUM FAMILY INCOME'] = df['MEDIUM FAMILY INCOME'].replace('\\$', '', regex=True).replace(',', '', regex=True)\n",
        "# Convert the column to float type\n",
        "df['MEDIUM FAMILY INCOME'] = df['MEDIUM FAMILY INCOME'].astype('int64')"
      ],
      "metadata": {
        "id": "kA-PZwKQuL2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I did the same thing here and swiched them to int64\n",
        "df['PCI'] = df['PCI'].replace('\\$', '', regex=True).replace(',', '', regex=True)\n",
        "\n",
        "df['PCI'] = df['PCI'].astype('int64')"
      ],
      "metadata": {
        "id": "xYZQktdXuS_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "-9cVrPcq2cma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['DEPENDENT VARIABLE'] = df['DEPENDENT VARIABLE'].astype('category')\n",
        "df['BALLOT TYPE'] = df['BALLOT TYPE'].astype('category')\n",
        "df['MSA'] = df['MSA'].astype('category')"
      ],
      "metadata": {
        "id": "y3TDguRnlBaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "4R7aPaowt1QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all numeric variables\n",
        "df.select_dtypes(include=['number'])"
      ],
      "metadata": {
        "id": "7qGUuceGnGd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all category variables\n",
        "df.select_dtypes(include=['category'])"
      ],
      "metadata": {
        "id": "iIkFSrkoZGcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I wanted to see if ppoverty level had an impact on their decision on voting yes or no and it is sitting at 42 rows for yes.\n",
        "df[(df['POVERTY LEVEL'] > 30) & (df['DEPENDENT VARIABLE']== 'Yes')]"
      ],
      "metadata": {
        "id": "DOmijlUZorBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And we are sitting at 26 rows for no.\n",
        "df[(df['POVERTY LEVEL'] > 30) & (df['DEPENDENT VARIABLE']== 'No')]"
      ],
      "metadata": {
        "id": "2O1MfTtwsfQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the variance, standard deviation, and range of a numeric varaible: MEDIUM FAMILY INCOME\n",
        "print(\"variance: \", df['MEDIUM FAMILY INCOME'].var(), \"standard deviation: \", df['MEDIUM FAMILY INCOME'].std(), \"range: \", df['MEDIUM FAMILY INCOME'].min(), df['MEDIUM FAMILY INCOME'].max())"
      ],
      "metadata": {
        "id": "Mf5P6hi7tuUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['MEDIUM FAMILY INCOME'].describe()"
      ],
      "metadata": {
        "id": "MCk-0wCraW3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the data preprocessing phase of my project, I focused on transforming data types and conducting analyses on poverty levels. By converting data types and exploring variations in poverty levels, I gained crucial insights that guided my approach for the remainder of the project."
      ],
      "metadata": {
        "id": "HV3qI3EgZtgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visulation"
      ],
      "metadata": {
        "id": "AGeq1R1Armqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section I will be preforming visulations to gain a better understanding of the data. Finding possible trends or things that might catch my eye insulting in further examination."
      ],
      "metadata": {
        "id": "ZpwRVmcMr5fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot of a numeric variable: MEDIUM FAMILY INCOME\n",
        "snsplot = sns.boxplot(x='MEDIUM FAMILY INCOME', data = df)\n",
        "snsplot.set_title(\"Boxplot of MEDIUM FAMILY INCOME\")"
      ],
      "metadata": {
        "id": "y3i941dxaiWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this visulation I wanted to see how a few variables were related to my target variable.\n",
        "correlation_matrix = df[[ 'DEPENDENT VARIABLE', 'BALLOT TYPE', 'POPULATION', 'PCI',\n",
        "                         'PERCENT MALE','POVERTY LEVEL','UNEMPLOYMENT RATE', 'AGE24',\n",
        "                          'AGE44']].corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True)\n",
        "\n",
        "plt.title('Correlation Matric for numeric features')\n",
        "\n",
        "plt.xlabel('For Features')\n",
        "\n",
        "plt.ylabel('Percent white Features')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m1hE6RT0sHyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this shows that wagering is more accepted compared to gambing.\n",
        "snsplot = sns.countplot(x='BALLOT TYPE', data=df)\n",
        "snsplot.set_title(\"ballot type who picked wagering or gambing\")"
      ],
      "metadata": {
        "id": "Npj6ZgDiw2zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x='POVERTY LEVEL', y='MSA', data=df)\n",
        "plt.xlabel('POVERTY LEVEL')\n",
        "plt.ylabel('MSA')\n",
        "plt.title('Boxplot of poverty level by MSA')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VHj_PBaGv6pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of a numeric variable: Unemployment Rate\n",
        "snsplot = sns.histplot(x='UNEMPLOYMENT RATE', data = df)\n",
        "snsplot.set_title(\"Histogram of Unemployment rate in data set\")"
      ],
      "metadata": {
        "id": "u-A-I0gEZNc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the data visualization section of my project, I employed various techniques to gain insights into the data. This included using heat maps, histograms specifically focusing on the unemployment rate, and box plots. These visualizations helped me analyze patterns, distributions, and correlations within the data, providing valuable insights for further analysis and model development."
      ],
      "metadata": {
        "id": "5r3it0UBxxJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model development"
      ],
      "metadata": {
        "id": "EgfIXTGtfbft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section I will be developing the models which will be 3 modles in total. I will be analizing these models and seeing which do better or worse. I will eloborate at the end."
      ],
      "metadata": {
        "id": "vGrlyCZ9fq6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ymNnSr-ic1-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=[ 'BALLOT TYPE', 'MSA'], drop_first=True)\n",
        "df"
      ],
      "metadata": {
        "id": "6IoNKQVoyZLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the porportion of target variable for data set\n",
        "target = df['DEPENDENT VARIABLE']\n",
        "print(target.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "h7VhXvCQfwEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition the data\n",
        "predictors = df.drop(['DEPENDENT VARIABLE'],axis=1)\n",
        "predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.3, random_state=0)\n",
        "print(predictors_train.shape, predictors_test.shape, target_train.shape, target_test.shape)"
      ],
      "metadata": {
        "id": "DRZLsBg8S_yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine predictors_train and target_train into a single DataFrame\n",
        "combined_train_df = pd.concat([predictors_train, target_train], axis=1)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_df = combined_train_df[combined_train_df['DEPENDENT VARIABLE'] == 'No']\n",
        "minority_df = combined_train_df[combined_train_df['DEPENDENT VARIABLE'] == 'Yes']\n",
        "\n",
        "# Undersample the majority class randomly\n",
        "undersampled_majority = majority_df.sample(n=len(minority_df), random_state=5)\n",
        "\n",
        "# Combine the undersampled majority class and the minority class\n",
        "undersampled_data = pd.concat([undersampled_majority, minority_df])\n",
        "\n",
        "# Shuffle the combined DataFrame to ensure randomness\n",
        "balanced_data = undersampled_data.sample(frac=1, random_state=5)\n",
        "\n",
        "# Split the balanced_data into predictors_train and target_train\n",
        "predictors_train = balanced_data.drop(columns=['DEPENDENT VARIABLE'])\n",
        "target_train = balanced_data['DEPENDENT VARIABLE']\\\n",
        "\n",
        "print(target_train.value_counts(normalize=True), target_train.shape)\n",
        "\n",
        "# Now the data is balanced!!"
      ],
      "metadata": {
        "id": "wACWHYtCciLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "oKmXzy5-jtKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree depth of 3"
      ],
      "metadata": {
        "id": "-Wuhl56XEq7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a decision tree model on training data with max_depth = 3\n",
        "\n",
        "model = DecisionTreeClassifier(criterion = \"entropy\", random_state=1, max_depth = 3)\n",
        "\n",
        "model.fit(predictors_train, target_train)"
      ],
      "metadata": {
        "id": "Qyb1U2uVk2KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the tree\n",
        "\n",
        "fig = plt.figure(figsize=(40,30))\n",
        "tree.plot_tree(model,\n",
        "               feature_names = list(predictors_train.columns),\n",
        "               class_names=['No','Yes'],\n",
        "               filled=True)"
      ],
      "metadata": {
        "id": "-F-YWFDxlHBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I will have the model make a prediction on test data.\n",
        "\n",
        "prediction_on_test = model.predict(predictors_test)"
      ],
      "metadata": {
        "id": "hHwMuyGvnFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: confusion_matrix\n",
        "cm = confusion_matrix(target_test, prediction_on_test)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_).plot()"
      ],
      "metadata": {
        "id": "QW0_Ea8znfB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: accuracy, precision, recall, and f1-score\n",
        "\n",
        "print(classification_report(target_test, prediction_on_test))\n"
      ],
      "metadata": {
        "id": "lT5NH4lmoE71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree depth of 5"
      ],
      "metadata": {
        "id": "kbMZrIzmE0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a decision tree model on training data with max_depth = 5\n",
        "\n",
        "model2 = DecisionTreeClassifier(criterion = \"entropy\", random_state=1, max_depth = 5)\n",
        "\n",
        "model2.fit(predictors_train, target_train)"
      ],
      "metadata": {
        "id": "2PrppacN0rnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the tree\n",
        "\n",
        "fig = plt.figure(figsize=(40,30))\n",
        "tree.plot_tree(model2,\n",
        "               feature_names = list(predictors_train.columns),\n",
        "               class_names=['No','Yes'],\n",
        "               filled=True)"
      ],
      "metadata": {
        "id": "XQOgeOS108iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I will have my second model2 make a prediction on test data.\n",
        "\n",
        "prediction_on_test = model2.predict(predictors_test)"
      ],
      "metadata": {
        "id": "QeatTcVA1IY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: confusion_matrix\n",
        "cm = confusion_matrix(target_test, prediction_on_test)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model2.classes_).plot()"
      ],
      "metadata": {
        "id": "iUHu-9Tv1dmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: accuracy, precision, recall, and f1-score\n",
        "\n",
        "print(classification_report(target_test, prediction_on_test))"
      ],
      "metadata": {
        "id": "YaicieJB1jnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these two modles it shows that going to a length of 5 imporves my model accuracy by 8% totaling at 86% which is huge just from increasing it by two rows. My precision did drop from model 1 (88) to modle 2 (86) however, this little drop helped my presicion on NO increase 13% showing my modle is better at identifying if someone voted yes or no. On recall it did decrease on no however, my Yes did increase so making each model better at catching who voted no and who voted yes."
      ],
      "metadata": {
        "id": "rSBqAwPp1zyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes model prediction"
      ],
      "metadata": {
        "id": "MzG_zUh_7xDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a Naive Bayes model on training data\n",
        "model_NB = MultinomialNB()\n",
        "model_NB.fit(predictors_train, target_train)"
      ],
      "metadata": {
        "id": "1P6L0G7b8YKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on testing data (0.5 points)\n",
        "prediction_on_test_NB = model_NB.predict(predictors_test)"
      ],
      "metadata": {
        "id": "VD_y6Bz68ihm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: confusion_matrix\n",
        "cm_NB = confusion_matrix(target_test, prediction_on_test_NB)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_NB, display_labels=model_NB.classes_).plot()\n",
        "#plot_confusion_matrix(model, predictors_test, target_test, cmap=plt.cm.Blues, values_format='d')"
      ],
      "metadata": {
        "id": "pmk-qzuR8zu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: accuracy, precision, recall, and f1-score.\n",
        "print(classification_report(target_test, prediction_on_test_NB))"
      ],
      "metadata": {
        "id": "OAu9HU-09SoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Naive Bayes model achieved an accuracy of 0.56, lower than the Decision Tree model. It showed good recall for class No (.84) but recall for class Yes fell short(0.23). The F1-score for class 1 was 0.32, indicating a need for improvement in balancing precision and recall. These means I need refining the model for better performance."
      ],
      "metadata": {
        "id": "CqvN4nkH_g4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **K Nearest Neighbor**"
      ],
      "metadata": {
        "id": "QG01qquJD6aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n_neighbors = 1"
      ],
      "metadata": {
        "id": "ygvShtARGnDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply minmax normalization on predictors\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "predictors_normalized = pd.DataFrame(min_max_scaler.fit_transform(predictors))\n",
        "predictors_normalized.columns = predictors.columns\n",
        "predictors_normalized"
      ],
      "metadata": {
        "id": "5UyS8qjJGAZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a K Nearest Neighbor model on training data with n_neighbors = 1\n",
        "model = KNeighborsClassifier(n_neighbors = 1)\n",
        "model.fit(predictors_train, target_train)"
      ],
      "metadata": {
        "id": "sFULTkPSGama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on training and testing data\n",
        "prediction_on_train = model.predict(predictors_train)\n",
        "prediction_on_test = model.predict(predictors_test)"
      ],
      "metadata": {
        "id": "D2e-Ua9PG2dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(target_train, prediction_on_train)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_).plot()"
      ],
      "metadata": {
        "id": "NthecCBSIAOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on training data: accuracy, precision, recall, and f1-score (1 points)\n",
        "print(classification_report(target_train, prediction_on_train ))\n",
        "\n",
        "# With this model I was expecting to get a perfect model because the model is esentally looking at itself and itself is 100% correct."
      ],
      "metadata": {
        "id": "nkt14kV_IJ-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: confusion_matrix\n",
        "cm = confusion_matrix(target_test, prediction_on_test)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_).plot()"
      ],
      "metadata": {
        "id": "feZYGhRbGzRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on testing data: accuracy, precision, recall, and f1-score (1 points)\n",
        "print(classification_report(target_test,prediction_on_test))\n",
        "# However it dose porly on test data which is the most importent."
      ],
      "metadata": {
        "id": "Fysa0xb2K1hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n_neighbors = 4"
      ],
      "metadata": {
        "id": "T-ZlWkWNQRDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a K Nearest Neighbor model on training data with n_neighbors = 3\n",
        "model2 = KNeighborsClassifier(n_neighbors = 3)\n",
        "model2.fit(predictors_train, target_train)"
      ],
      "metadata": {
        "id": "Jwojje6wMXOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on training and testing data\n",
        "prediction_on_train = model2.predict(predictors_train)\n",
        "prediction_on_test = model2.predict(predictors_test)"
      ],
      "metadata": {
        "id": "UA3VR5F9Mlyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(target_test, prediction_on_test)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_).plot()"
      ],
      "metadata": {
        "id": "S5b2DDWhMoo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the evaluation results on test data: accuracy, precision, recall, and f1-score\n",
        "print(classification_report(target_test, prediction_on_test ))"
      ],
      "metadata": {
        "id": "Qzd61dTCMtXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my KNN model I yet again recieved lower accuracy compared to my decision tree how ever it increased compared to my Naive Bayes model. All of my categorys increased when I increased k = 3 however, when I increased it more things seem to drop. So there is still some tweaking I need to work out."
      ],
      "metadata": {
        "id": "1k3DkH6WY919"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Model Evaluation"
      ],
      "metadata": {
        "id": "gFppgUcumfGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of all the machine learning models I tested, the decision tree emerged as the star player. Showcasing its ability to unveil intricate data patterns with an outstanding accuracy of 85%. Impressively, it consistently scored 80% or higher in recall, precision, and F1-score metrics. However, the same cannot be said for the other models. My Naive Bayes model struggled, yielding a mere 56% accuracy and a dismal 23% recall for positive votes. Lastly, the KNN model showed improvement, not enough to celebrate. It reached an accuracy of 60% after increasing K to 4. Although its precision in classifying negitive votes was better compared to positive votes there is still work needed to enhance this model further."
      ],
      "metadata": {
        "id": "CdiGnA_lnOry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        " In conclusion, this project delved into machine learning algorithms to predict voting behavior. The Decision Tree model emerged as the standout performer, showcasing high accuracy and strength in capturing intricate data patterns. On the other hand, Naive Bayes struggled with accuracy and recall, highlighting its limitations. The K-Nearest Neighbors model showed promise after tuning but requires further refinement. Overall, this project emphasizes  the importance of selecting the right algorithm for specific data contexts and objectives, providing valuable insights for future predictive modeling endeavors."
      ],
      "metadata": {
        "id": "acQizPBpsOIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/Colab Notebooks/IncremonaBrandonUse.ipynb\""
      ],
      "metadata": {
        "id": "2rhrEkQuzwY4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}